'use strict';

const fs = require('fs');

const child_process = require('child_process');

const mkdirp = require('mkdirp'); //for dc2
const path = require('path');
const archiver = require('archiver');
const ssh2 = require('ssh2');
const axios = require('axios');
const request = require('request'); //deprecate!
const zlib = require('zlib');
const async = require('async');

exports.mongodb = "mongodb://brainlife_mongodb/warehouse";

exports.debug = true; 

//used to post/poll health status from various services
exports.redis = { url: "redis://brainlife_redis" };

//get it from https://github.com/settings/tokens  
//exports.github = { access_token: "..." };

//admin+client scope oauth2 token used to invite users
//once you create oauth app, you will need to re-authorize it with admin+client (it gave with just admin scope)
//https://slack.com/oauth/authorize?&client_id=(your client id)&team=(teamId)&install_redirect=install-on-team&scope=admin+client
//more information can be found here https://github.com/outsideris/slack-invite-automation
exports.slack = {
    token: "get it from slack",
    newuser: "dev", //channel to announce newuser registration
}

exports.amaretti = {
    api: "http://brainlife_amaretti-api:8080",
}
//exports.wf = exports.amaretti; //deprecated (use amaretti)

exports.ipstack = {
    token: "is this still used?",
}
exports.mailchimp = {
    api_key: "get it from mailchim",
    newsletter_list: "8d07cef694", //list ID to subscribe newusers to
}

exports.auth = {
    api: "http://brainlife_auth-api:8080",
}

exports.warehouse = {
    //used by rule handler to submit dataset download request
    api: "http://brainlife_warehouse-api:8080",

    url: "https://localhost.brainlife.io", //to test datacite

    //used to issue warehouse token to allow dataset download
    public_key: fs.readFileSync(__dirname+'/warehouse.pub'),
    private_key: fs.readFileSync(__dirname+'/warehouse.key'),

    //place to store rule logs 
    rule_logdir: "/tmp",

    //jwt used to access other services
    //submit task on amaretti
    //query gids from auth
    jwt: fs.readFileSync(__dirname+'/warehouse.jwt', 'ascii').trim(),

    //configuration encryption key
    configEncryption: {
        algorithm: "aes-256-cbc",
        secret: "here is my password123",
    },
}

exports.mail = {
    from: "brainlife.io <brlife@iu.edu>",

    //node mailer config
    mailer: {
        host: 'mail-relay.iu.edu', //max recipients per email: 30
        secure: true, //port 465
        auth: {
            user: 'mememe',
            pass: 'passpasspss',
        },
        pool: true, //use connection pool
    },
}

//deprecated by influxdb
exports.metrics = {
    counts: {
        path: "/tmp/warehouse.24m.metrics", 
        prefix: "dev.warehouse", 
        interval: 3600*24, 
    },

    health_counts: {
        path: "/tmp/warehouse.5m.metrics", 
        prefix: "dev.warehouse", 
        interval: 60*5, 
    },

    service_prefix: "dev.amaretti.service",

    api: "http://10.0.0.10:2080", //TODO
}

exports.influxdb = {
    connection: {
        url: "http://brainlife_influxdb:8086",
        token: "mydevtoken",
    },
    org: "brainlife",
    bucket: "brainlife",
    location: "localhost",

    countInterval: 10*1000, 
    healthInterval: 10*1000, 
}

//for event handler
exports.event = {
    amqp: {
        url: "amqp://guest:guest@brainlife_rabbitmq:5672/brainlife",

        //collected by cron
        queues: "/tmp/rabbitmq.queues.json",
    },
}

//for rule handler
exports.rule = {
    max_task_per_rule: 30, //limit number of concurrently running tasks submission
    nice: 10, //default nice value for stage/process tasks 
}

exports.express = {
    host: "0.0.0.0",
    port: 8080,

    //public key used to validate jwt token
    pubkey: fs.readFileSync(__dirname+'/auth.pub', 'ascii').trim(),
}

/*
exports.datacite = {
    prefix: "10.0322/bldev.",  //test account
    username: "DATACITE.BL",
    password: fs.readFileSync(__dirname+'/datacite.password', {encoding: "ascii"}).trim(),
    api: "https://mds.test.datacite.org",
}
*/

//https://msr-apis.portal.azure-api.net/developer
exports.mag = {
    subscriptionKey: "get mag key",
}

///////////////////////////////////////////////////////////////////////////////////////////////////
//
//  storage system where we can archive data
//
exports.storage_systems = {};

exports.archive = {
    storage_default: "local",
    gid: 2, //warehouse group enabled to access storage service
}

exports.geocode = {
    apikey: "get google api geocode key",
}

function getLocalPath(dataset) {
    let _path = "/archive/"; //not the same as secondary
    if(!dataset) return _path; //why?
    return _path + dataset.project+"/"+dataset._id+".tar";
}

exports.storage_systems.local = {
    need_backup: false,
    test: cb=>{
        //TODO
        cb();
    },

    stat: (dataset, cb)=>{
        const path = getLocalPath(dataset);
        const stat = fs.statSync(path);
        cb(null, stat);
    },

    download: (dataset, cb)=>{
        const path = getLocalPath(dataset);
        const stream = fs.createReadStream(path);
        const filename = path.split("/").pop();
        cb(null, stream, filename);
    },
}

//group analysis 
exports.groupanalysis = {
    gid: 3, //group id that allows access to ga resource

    secondaryDir: "/secondary", 

    getSecondaryDownloadStream(path, cb) {
        //make sure file exists
        fs.stat(path, (err,stats)=>{
            if(err)  return cb(err);

            //now stream!
            let stream = fs.createReadStream(path);
            stream.on('error', err=>{
                console.error(err);
            });
            stream.on('close', code=>{
                //console.log("stream closed.. ending connection", code);
                //conn.end();
            });

            cb(null, stream);
        });
    },

    getSecondaryUploadStream(path, cb) {
        const dname = path.dirname(path);
        fs.mkdirSync(dname);
        cb(null, fs.createWriteStream(path));
    },
}


/*
function connect_dc(cb) {
    var conn = new ssh2.Client();
    conn.on('ready', ()=>{
        cb(null, conn); 
    });
    conn.on('error', err=>{
        cb(err);
    });
    try {
        console.log("connecting to dc");
        conn.connect({
            username: "brlife",
            host: "carbonate.uits.iu.edu",
            privateKey: fs.readFileSync(__dirname+'/local.id_rsa'),
        });
    } catch(err) {
        cb(err);
    }
}
*/

/*
//TODO - I should try connection queue agagin
//(not used anymore)
function connect_wrangler(cb) {
    var conn = new ssh2.Client();
    conn.on('ready', function() {
        console.log("connected to wrangler - opening sftp stream also");
        conn.sftp((err, sftp)=>{
            if(err) return cb(err);
            if(cb) cb(null, conn, sftp);
            cb = null;
        });
    });
    conn.on('end', function() {
        console.log("wrangler connection ended");
    });
    conn.on('close', function() {
        console.log("wrangler connection closed");
    });
    conn.on('error', function(err) {
        console.error("wrangler connectionn error");
        //we want to return connection error to caller, but error could fire after ready event is called.
        //like timeout, or abnormal disconnect, etc..  need to prevent calling cb twice!
        if(cb) cb(err);
        cb = null;
    });

    conn.connect({
        username: "brlife",
        host: "149.165.169.130", //wrangler2
        privateKey: fs.readFileSync(__dirname+'/local.id_rsa'),
        keepaliveInterval: 10*1000, //default 0 (disabled)
    });
}
*/

/*
function getOSNPath(dataset) {
    let _path = "dev/"; //not the same as secondary
    if(!dataset) return _path;
    _path += dataset.project+"/";
    _path += dataset._id+".tar";
    return _path;
}

const osn = require('./osnConfig');
exports.storage_systems.osn = {
    need_backup: true, 
    test: cb=>{
        //test uploading something..
        osn.s3.upload({Bucket: osn.bucket, Key: 'test.txt', Body: 'test'}, err=>{
            if(err) return cb(err);
            cb();
        });
    }, 

    stat: (dataset, cb)=>{
        const path = getOSNPath(dataset);
        osn.s3.headObject({Bucket: osn.bucket, Key: path}, (err, metadata)=>{
            if(err) return cb(err);
            const stats = {
                size: metadata.ContentLength,
            }
            cb(null, stats);
        });
    },

    download: (dataset, cb)=>{
        const path = getOSNPath(dataset);
        const filename = path.split("/").pop();
        const stream = osn.s3.getObject({Bucket: osn.bucket, Key: path}).createReadStream();
        cb(null, stream, filename);
    },

    remove: (dataset, cb)=>{
        cb("todo");
    }
}
*/

/*
//TODO - I should try connection queue agagin
function connect_osiris(cb) {
    var conn = new ssh2.Client();
    conn.on('ready', function() {
        console.log("connected to osiris - opening sftp stream also");
        conn.sftp((err, sftp)=>{
            if(err) console.error(err);
            if(cb) cb(err, conn, sftp);
            cb = null;
        });
    });
    conn.on('end', function() {
        console.log("osiris connection ended");
    });
    conn.on('close', function() {
        console.log("osiris connection closed");
    });
    conn.on('error', function(err) {
        console.error("osiris connectionn error:"+err);
        //we want to return connection error to caller, but error could fire after ready event is called.
        //like timeout, or abnormal disconnect, etc..  need to prevent calling cb twice!
        if(cb) cb(err);
        cb = null;
    });

    conn.connect({
        username: "shayashi",
        host: "msu-xfer01.osris.org", //Timed out while waiting for handshake?
        privateKey: fs.readFileSync(__dirname+'/local.id_rsa'),
        keepaliveInterval: 10*1000, //default 0 (disabled)
        //keepaliveCountMax: 30, //default 3 (https://github.com/mscdex/ssh2/issues/367)
    });
}

function get_osiris_archive_path(dataset) {
    let _path = "/user/shayashi/brainlife/dev-archive/"; //not the same as secondary
    if(!dataset) return _path;
    _path += dataset.project+"/";
    _path += dataset._id+".tar";
    return _path;
}
*/

/*
exports.storage_systems.osiris = {
    need_backup: true, //need_backup if it's our storage (not openneuro)
    test: cb=>{
        connect_osiris((err, conn, sftp)=>{
            if(err) return cb(err);
            sftp.stat(get_osiris_archive_path(), (err,stats)=>{
                conn.end();
                //TODO - check what's in stat?
                cb(err);
            });
        });
        //fs.access(get_osiris_archive_path(), cb);
    }, 

    stat: (dataset, cb)=>{
        //fs.stat(get_osiris_archive_path(dataset), cb);
        connect_osiris((err, conn, sftp)=>{
            if(err) return cb(err);
            sftp.stat(get_osiris_archive_path(dataset), (err,stat)=>{
                conn.end();
                cb(err, stat);
            });
        });
    },

    download: (dataset, cb)=>{
        console.log("osiris/download called");
        connect_osiris((err, conn, sftp)=>{
            //console.log("------------------------");
            //console.log(err);
            //console.log(conn);
            //console.log(sftp);
            if(err) return cb(err);
            let path = get_osiris_archive_path(dataset);
            let filename = path.split("/").pop();
            //console.debug(["loading...", path, filename]);
            console.log("streaming from", path)

            //TODO why not use sftp?
            conn.exec("cat "+path, (err, stream)=>{
                stream.on('error', err=>{
                    console.error("stream failed but train has already left the station", err);
                });
                stream.on('close', code=>{
                    console.log("stream clsoed.. closing connection");
                    conn.end();
                });
                //I believe listening to stream is required for close event to fire..?
                stream.stderr.on('data', data=>{
                    console.error(data.toString());
                });
                cb(err, stream, filename);
            });
        });
    },

    remove: (dataset, cb)=>{
        connect_osiris((err, conn, sftp)=>{
            if(err) return cb(err);
            let _path = get_osiris_archive_path(dataset);
            console.log("rm -f "+_path);
            conn.exec("rm -f "+_path, (err, stream)=>{
                if(err) {
                    conn.end();
                    return cb(err);
                }
                stream.on('error', err=>{
                    console.error(err);
                });
                stream.on('close', (code, signal)=>{
                    conn.end(); 
                    if(code == 0) return cb(null);
                    else cb("rm failed with code:"+code);
                });
                stream.on('data', data=>{
                    console.log(data.toString());
                });
                stream.stderr.on('data', data=>{
                    console.error(data.toString());
                });
            });
        });
    }
} 
*/

/*
exports.storage_systems.project = {
    need_backup: true, //need_backup if it's our storage (not openneuro)
    test: cb=>{
        console.log("project/test - todo");
        cb();
    },

    stat: (dataset, cb)=>{
        cb();
    },

    download: (dataset, cb)=>{
        let resource_id = dataset.storage_config.resource_id; 
        let path = dataset.project+"/"+dataset._id+".tar";
        axios({
            method: 'get',
            url: exports.amaretti.api+"/resource/archive/download/"+resource_id+"/"+path, 
            responseType: 'stream',
            headers: {Authorization: "Bearer "+ exports.warehouse.jwt}
        }).then(res=>{
            cb(null, res.data, dataset._id+".tar");
        });
    },
}

function get_copysource_dataset(dataset) {
    return Object.assign({}, dataset, {
        project: dataset.storage_config.project,
        storage: dataset.storage_config.storage,
        storage_config: dataset.storage_config.storage_config,
        _id: dataset.storage_config.dataset_id,
    });
}

exports.storage_systems.copy = {
    need_backup: true,
    test: cb=>{
        cb();
    }, 

    stat: (dataset, cb)=>{
        let source_dataset = get_copysource_dataset(dataset);
        let system = exports.storage_systems[source_dataset.storage];
        system.stat(source_dataset, cb);
    },

    //TODO - with app-archive in place, I don't think we use this anymore
    upload: (dataset, cb)=>{
        cb("can't upload to copy target");
    },

    download: (dataset, cb)=>{
        let source_dataset = get_copysource_dataset(dataset);
        let system = exports.storage_systems[source_dataset.storage];
        system.download(source_dataset, (err, stream, filename)=>{

            //use the original dataset id as filename (but keep the .tar vs .tar.gz intact)
            let copy_filename = filename.split(".");
            copy_filename[0] = dataset._id;
            cb(err, stream, copy_filename.join("."));
        });
    },
} 
*/

/*
exports.storage_systems["dcwan/hcp"] = {
    test: cb=>{
        connect_dc((err, conn)=>{
            if(err) return cb(err);
            conn.end();
            cb();
        });
    }, 
    stat: (dataset, cb)=>{
        //TODO..
        cb();
    },
    upload: (dataset, cb)=>{
        cb("no upload to dcwan/hcp");
    },
    download: (dataset, cb)=>{
        connect_dc((err, conn)=>{
            if(err) return cb(err);
            if(dataset.storage_config.files) {
                 conn.sftp((err, sftp)=>{
                    if (err) {
                        conn.end();
                        return cb(err);
                    }

                    //TODO - archiver seems to leak memory really bad.. 
                    //I can't really recreate the problem, but I think it happens if more data is fed to archiver
                    //than archiver can stream it out - using some kind of an internal buffer that never gets released
                    var archive = archiver('tar');
                    dataset.storage_config.files.forEach(file=>{
                        var stream = sftp.createReadStream(file.filepath);
                        stream.on('error', err=>{
                            console.error("stream failed but train has already left the station", file.filepath, err);
                        });
                        stream.on('end', ()=>{
                            console.log("stream ended", file.filepath);
                        });
                        console.log("archiver downloading", file.filepath, file.local);
                        archive.append(stream, {name: file.local});
                    });
                    archive.on('finish', ()=>{
                        console.log("archive finished");
                    });
                    archive.on('end', ()=>{
                        console.log("archive ended.. conn.end()");
                        conn.end();
                    });
                    archive.on('error', err=>{
                        console.log("archive error.......................");
                        console.error(err);
                    });
                    console.log("archive finalized");
                    archive.finalize();
                    cb(null, archive, dataset._id+".tar");
                });
            }

            if(dataset.storage_config.dirpath) {
                var local = dataset.storage_config.local;
                conn.exec("cd "+dataset.storage_config.dirpath+" && tar hc * --transform 's|^|/"+local+"/|'", (err, stream)=>{
                    if(err) return cb(err);
                    stream.on('close', code=>{
                        //console.log("done with tar stream - closing connection:",code)
                        conn.end();
                    });
                    cb(null, stream, dataset._id+".tar");
                });
            }
        });
    },
}
*/

/*
exports.storage_systems.url = {
    need_backup: false,
    test: cb=>{
        //TODO - maybe check to see if we have outbound connection?
        cb();
    }, 

    stat: (dataset, cb)=>{
        //no stats as .tar will be generated on the fly
        cb(null);
    },

    upload: (dataset, cb)=>{
        cb("read only");
    },

    download: (dataset, cb)=>{
        let archive = archiver('tar');
        let gzip = zlib.createGzip();
        dataset.storage_config.files.forEach(file=>{
            let stream = request(file.url);
            if(file.url.endsWith(".nii")) {
                //compress .nii with .nii.gz as *all* brainlife datatype uses .nii.gz for nifti
                stream = stream.pipe(gzip);
            }
            archive.append(stream, {name: file.local});
        });
        archive.finalize();
        cb(null, archive, dataset._id+".tar");
    },
}
*/

/*
async function walk(dir) {
    let files = await fs.promises.readdir(dir);
    files = await Promise.all(files.map(async file => {
        const filePath = path.join(dir, file);
        const stats = await fs.promises.stat(filePath);
        if (stats.isDirectory()) return walk(filePath);
        else if(stats.isFile()) return filePath;
    }));
    return files.reduce((all, folderContents) => all.concat(folderContents), []);
}

const datalad_root = "/mnt/datalad";
exports.storage_systems.datalad = {
    need_backup: false,
    test: cb=>{
        //TODO??
        cb();
    }, 

    stat: (dataset, cb)=>{
        //no stats as .tar will be generated on the fly
        cb(null);
    },

    upload: (dataset, cb)=>{
        cb("read only");
    },

    download: (dataset, cb)=>{
        connect_wrangler((err, conn, sftp)=>{
            if (err) {
                conn.end();
                return cb(err);
            }

            //do datalad get first
            async.each(dataset.storage_config.files, (file, next_file)=>{
                let cwd = datalad_root;
                let src_tokens = file.src.split("/")

                //for(let i = 0;i < 2; ++i) {
                //    cwd += "/"+src_tokens.shift();
                //}
                //let src_sub = src_tokens.join("/");

                dataset.storage_config.path.split("/").forEach(t=>{
                        src_tokens.shift();
                        cwd+= "/"+t;
                });
                let src_sub = src_tokens.join("/");

                console.debug("datalad get "+file.src);
                console.debug("cd "+cwd);
                console.debug("datalad get "+src_sub);

                conn.exec("cd "+cwd+" && datalad get "+src_sub, (err, stream)=>{
                    if(err) return next_file(err);
                    stream.on('error', err=>{
                        console.error(err);
                    });
                    stream.on('close', code=>{
                        console.debug("done with "+file.src);
                        next_file();
                    });
                    stream.on('data', data=>{
                        console.debug(data.toString());
                    });
                    //I believe listening to stream is required for close event to fire..?
                    stream.stderr.on('data', data=>{
                        console.error(data.toString());
                    });
                });
            }, err=>{
                if(err) {
                    conn.end();
                    return cb(err);
                }
                
                console.debug("creating tar ball");

                let archive = archiver('tar');
                let gzip = zlib.createGzip();
                archive.on('end', ()=>{
                    console.log("archive ended.. conn.end()");
                    conn.end();
                });
                async.forEach(dataset.storage_config.files, (file, next_file)=>{
                    let source_path = datalad_root+"/"+file.src;
                    sftp.stat(source_path, (err, stat)=>{
                        if(err) {
                            conn.end();
                            return cb(err);
                        }
                        if(stat.isDirectory()) {
                            //I need to deference symbolic link as archiver won't deference symlinks
                            walk(source_path).then(entries=>{
                                entries.forEach(entry=>{
                                    let subpath = entry.substring(source_path.length);
                                    console.log(entry, file.dest+subpath);
                                    archive.append(sftp.createReadStream(entry), {name: file.dest+subpath});
                                });
                                next_file();
                            });
                            //archive.directory(source_path, file.dest);
                        } else {
                            let stream = sftp.createReadStream(source_path);
                            if(file.src.endsWith(".nii")) {
                                //compress .nii with .nii.gz as *all* brainlife datatype uses .nii.gz for nifti
                                console.log("passing", file.src, "through gzip stream");
                                stream = stream.pipe(gzip);
                            }
                            archive.append(stream, {name: file.dest});
                            next_file();
                        }
                    });
                }, err=>{
                    archive.finalize();
                    cb(err, archive, dataset._id+".tar");
                });

            });

        });
   },
}
*/

/*
exports.storage_systems.xnat = {
    need_backup: false,
    test: cb=>{
        console.log("xnat storage test - todo");
        cb();
    },

    stat: (dataset, cb)=>{
        cb();
    },

    upload: (dataset, cb)=>{
        cb("TODO..");
    },

    download: (dataset, cb)=>{
        const secretEnc = new Buffer(dataset.storage_config.secretEnc, 'base64');
        const password = child_process.execSync("openssl rsautl -inkey ./api/config/configEncrypt.key -decrypt", {
            input: secretEnc,
        }).toString();
        const config = dataset.storage_config;
        const subject = dataset.meta.subject;
        const experiment = dataset.meta.session;
        const scan = dataset.meta.xnat_scan||dataset._id;

        //"nosession" must be set in app-archive / warehouse/config.index.js
        const url = config.hostname+"/data/projects/"+config.project+"/subjects/"+subject+
            "/experiments/"+(experiment||"nosession")+
            "/scans/"+scan+"/"+config.path;

        //url += "/files";
        //url += "/resources/DICOM/files";
        //url += "/resources/DICOM/files";

        console.log("downloading from ", url);

        axios({
            method: 'get',
            responseType: 'stream',
            url,
            auth: {
                username: dataset.storage_config.token,
                password,
            },
            params: {
                format: "zip", //zstd?
            },
        }).then(res=>{
            //cb(null, res.data, dataset._id+".DICOM.zip");
            cb(null, res.data, dataset._id+".zip");
        });
    },
}
*/